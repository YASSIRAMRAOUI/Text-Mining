{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "399abb1a",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/YASSIRAMRAOUI/Text-Mining/blob/main/Atelier_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec92b328",
   "metadata": {},
   "source": [
    "# **Atelier 4 : Clustering et D√©tection de Plagiat**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0a9f5b",
   "metadata": {},
   "source": [
    "# **üì¶ 1. Pr√©paration de l'environnement et chargement des donn√©es**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb48d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des librairies n√©cessaires\n",
    "!pip install gdown scikit-learn pandas numpy matplotlib seaborn nltk\n",
    "\n",
    "# Importation des biblioth√®ques\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from glob import glob\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.cluster import KMeans, SpectralClustering, DBSCAN, OPTICS\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# T√©l√©chargement des ressources NLTK\n",
    "nltk.download('all', quiet=True)\n",
    "\n",
    "print(\"‚úÖ Environnement pr√™t!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6663a029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T√©l√©chargement du dataset\n",
    "!gdown --id 1j1OSM0LTJEj6VAfed7-UUQTGoiVMJ2If -O corpus.zip\n",
    "!unzip -q corpus.zip -d /content/\n",
    "\n",
    "print(\"‚úÖ Dataset t√©l√©charg√© et extrait!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e382d258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des donn√©es dans des DataFrames\n",
    "d1 = {'tasks': [], 'groups': [], 'persons': [], 'resp': [], 'filename': []}\n",
    "files = glob(\"/content/corpus-20090418/g*task*.txt\")\n",
    "\n",
    "for file in sorted(files):\n",
    "    f = open(file, \"r\", errors=\"ignore\")\n",
    "    task = file[len(file.split(\".\")[0])-1]\n",
    "    d1[\"tasks\"].append(task)\n",
    "    group = file[len(file.split(\".\")[0])-9]\n",
    "    d1[\"groups\"].append(group)\n",
    "    user = file[len(file.split(\".\")[0])-7]\n",
    "    d1[\"persons\"].append(user)\n",
    "    resp = f.read()\n",
    "    d1[\"resp\"].append(resp)\n",
    "    d1[\"filename\"].append(file.split('/')[-1])\n",
    "\n",
    "d2 = {'tasks': [], 'resp': [], 'filename': []}\n",
    "files = glob(\"/content/corpus-20090418/orig*.txt\")\n",
    "\n",
    "for file in sorted(files):\n",
    "    f = open(file, \"r\", errors=\"ignore\")\n",
    "    task = file[len(file.split(\".\")[0])-1]\n",
    "    d2[\"tasks\"].append(task)\n",
    "    resp = f.read()\n",
    "    d2[\"resp\"].append(resp)\n",
    "    d2[\"filename\"].append(file.split('/')[-1])\n",
    "\n",
    "tasks_pers = pd.DataFrame(data=d1)\n",
    "tasks_orig = pd.DataFrame(data=d2)\n",
    "\n",
    "print(\"üìä Statistiques du corpus:\")\n",
    "print(f\"Nombre de r√©ponses d'√©tudiants: {len(tasks_pers)}\")\n",
    "print(f\"Nombre de r√©ponses originales: {len(tasks_orig)}\")\n",
    "print(f\"\\nT√¢ches disponibles: {sorted(tasks_pers['tasks'].unique())}\")\n",
    "print(f\"Groupes: {sorted(tasks_pers['groups'].unique())}\")\n",
    "print(f\"Personnes par groupe: {sorted(tasks_pers['persons'].unique())}\")\n",
    "\n",
    "print(\"\\nüìÑ Aper√ßu des donn√©es:\")\n",
    "print(tasks_pers.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ccbd4a",
   "metadata": {},
   "source": [
    "# **üîß 2. Pr√©traitement et Vectorisation TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ba8d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Pr√©traitement du texte: tokenization, lemmatization, suppression des stopwords\"\"\"\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Initialisation du lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Stopwords anglais\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Lemmatization et filtrage\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens \n",
    "              if token.isalnum() and token not in stop_words and len(token) > 2]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def tfidf_vectorization():\n",
    "    \"\"\"Vectorisation TF-IDF du corpus complet\"\"\"\n",
    "    # Concat√©nation de tous les documents\n",
    "    corpus = pd.concat([tasks_pers[\"resp\"], tasks_orig[\"resp\"]], ignore_index=True)\n",
    "    \n",
    "    # Pr√©traitement\n",
    "    print(\"‚è≥ Pr√©traitement des documents...\")\n",
    "    corpus_preprocessed = [preprocess_text(doc) for doc in corpus]\n",
    "    \n",
    "    # Vectorisation TF-IDF\n",
    "    print(\"‚è≥ Vectorisation TF-IDF...\")\n",
    "    vectorizer = TfidfVectorizer(max_features=1000, min_df=2, max_df=0.8)\n",
    "    vectors = vectorizer.fit_transform(corpus_preprocessed)\n",
    "    \n",
    "    print(f\"‚úÖ Vectorisation termin√©e: {vectors.shape[0]} documents, {vectors.shape[1]} features\")\n",
    "    \n",
    "    return vectors, vectorizer, corpus_preprocessed\n",
    "\n",
    "# Ex√©cution de la vectorisation\n",
    "vectors, vectorizer, corpus_preprocessed = tfidf_vectorization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc59c99",
   "metadata": {},
   "source": [
    "# **üìä 3. Clustering avec K-Means**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6317c1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R√©duction de dimensionnalit√© avec PCA pour visualisation\n",
    "print(\"‚è≥ R√©duction de dimensionnalit√© avec PCA...\")\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "vectors_2d = pca.fit_transform(vectors.toarray())\n",
    "\n",
    "# Application de K-Means avec 5 clusters (une pour chaque t√¢che originale)\n",
    "print(\"\\n‚è≥ Application de K-Means...\")\n",
    "km = KMeans(n_clusters=5, init='k-means++', n_init=20, max_iter=300, random_state=42)\n",
    "km.fit(vectors_2d)\n",
    "y_kmeans = km.predict(vectors_2d)\n",
    "\n",
    "# Calcul des m√©triques de qualit√©\n",
    "silhouette = silhouette_score(vectors_2d, y_kmeans)\n",
    "calinski = calinski_harabasz_score(vectors_2d, y_kmeans)\n",
    "davies_bouldin = davies_bouldin_score(vectors_2d, y_kmeans)\n",
    "\n",
    "print(f\"\\nüìà M√©triques de qualit√© du clustering K-Means:\")\n",
    "print(f\"  - Silhouette Score: {silhouette:.4f} (plus proche de 1 = meilleur)\")\n",
    "print(f\"  - Calinski-Harabasz Score: {calinski:.4f} (plus √©lev√© = meilleur)\")\n",
    "print(f\"  - Davies-Bouldin Score: {davies_bouldin:.4f} (plus bas = meilleur)\")\n",
    "print(f\"  - Inertie (score): {km.score(vectors_2d):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acce0319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des clusters\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1], c=y_kmeans, s=50, cmap='viridis', alpha=0.6)\n",
    "centers = km.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=300, alpha=0.8, marker='X', edgecolors='black', linewidth=2)\n",
    "\n",
    "# Num√©roter chaque centro√Øde\n",
    "for i, (x, y) in enumerate(centers):\n",
    "    plt.text(x, y, str(i), fontsize=14, ha='center', va='center',\n",
    "             color='white', weight='bold')\n",
    "\n",
    "plt.title('Clustering K-Means - D√©tection de Plagiat', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Composante Principale 1', fontsize=12)\n",
    "plt.ylabel('Composante Principale 2', fontsize=12)\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüéØ Attribution des clusters: {y_kmeans}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193370cc",
   "metadata": {},
   "source": [
    "# **üîç 4. Exercice 1: Analyse des clusters et calcul de similarit√©**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2b338e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organisation des documents par cluster\n",
    "clusters_dict = {}\n",
    "for i in range(5):\n",
    "    cluster_indices = [idx for idx, x in enumerate(y_kmeans) if x == i]\n",
    "    clusters_dict[f'Cluster_{i}'] = cluster_indices\n",
    "\n",
    "# Affichage d√©taill√© des clusters\n",
    "print(\"=\" * 80)\n",
    "print(\"üìã COMPOSITION DES CLUSTERS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "total_docs = len(tasks_pers) + len(tasks_orig)\n",
    "num_student_docs = len(tasks_pers)\n",
    "\n",
    "for cluster_name, indices in clusters_dict.items():\n",
    "    print(f\"\\n{cluster_name}: {len(indices)} documents\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # S√©parer les documents originaux et √©tudiants\n",
    "    original_docs = [i for i in indices if i >= num_student_docs]\n",
    "    student_docs = [i for i in indices if i < num_student_docs]\n",
    "    \n",
    "    if original_docs:\n",
    "        print(f\"  üìÑ Documents originaux: {len(original_docs)}\")\n",
    "        for idx in original_docs:\n",
    "            orig_idx = idx - num_student_docs\n",
    "            print(f\"     ‚Üí Task {tasks_orig.iloc[orig_idx]['tasks']}: {tasks_orig.iloc[orig_idx]['filename']}\")\n",
    "    \n",
    "    if student_docs:\n",
    "        print(f\"  üë• Documents √©tudiants: {len(student_docs)}\")\n",
    "        # Grouper par t√¢che\n",
    "        tasks_in_cluster = {}\n",
    "        for idx in student_docs:\n",
    "            task = tasks_pers.iloc[idx]['tasks']\n",
    "            if task not in tasks_in_cluster:\n",
    "                tasks_in_cluster[task] = []\n",
    "            tasks_in_cluster[task].append(idx)\n",
    "        \n",
    "        for task, doc_indices in sorted(tasks_in_cluster.items()):\n",
    "            print(f\"     ‚Üí Task {task}: {len(doc_indices)} documents\")\n",
    "            for idx in doc_indices[:5]:  # Montrer les 5 premiers\n",
    "                row = tasks_pers.iloc[idx]\n",
    "                print(f\"        ‚Ä¢ Group {row['groups']}, Person {row['persons']}: {row['filename']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d23142f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de la similarit√© cosinus entre tous les documents\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìê CALCUL DE SIMILARIT√â COSINUS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calcul de la matrice de similarit√©\n",
    "similarity_matrix = cosine_similarity(vectors)\n",
    "\n",
    "def classify_plagiarism(similarity_score):\n",
    "    \"\"\"Classifie le type de plagiat selon le score de similarit√©\"\"\"\n",
    "    if similarity_score >= 0.70:\n",
    "        return \"Cut (70-100%)\"\n",
    "    elif similarity_score >= 0.40:\n",
    "        return \"Heavy (40-70%)\"\n",
    "    elif similarity_score >= 0.10:\n",
    "        return \"Light (10-40%)\"\n",
    "    else:\n",
    "        return \"Non (<10%)\"\n",
    "\n",
    "# Analyse de la similarit√© pour chaque cluster\n",
    "for cluster_name, indices in clusters_dict.items():\n",
    "    print(f\"\\n{cluster_name}:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Trouver le document original dans ce cluster\n",
    "    num_student_docs = len(tasks_pers)\n",
    "    original_docs = [i for i in indices if i >= num_student_docs]\n",
    "    student_docs = [i for i in indices if i < num_student_docs]\n",
    "    \n",
    "    if original_docs and student_docs:\n",
    "        orig_idx = original_docs[0]\n",
    "        orig_task_idx = orig_idx - num_student_docs\n",
    "        original_task = tasks_orig.iloc[orig_task_idx]['tasks']\n",
    "        \n",
    "        print(f\"üìÑ Document original: Task {original_task}\")\n",
    "        print(f\"üë• Analyse de {len(student_docs)} documents √©tudiants:\\n\")\n",
    "        \n",
    "        # Calculer la similarit√© de chaque document √©tudiant avec l'original\n",
    "        similarities = []\n",
    "        for student_idx in student_docs[:10]:  # Limiter √† 10 pour l'affichage\n",
    "            sim_score = similarity_matrix[orig_idx][student_idx]\n",
    "            student_info = tasks_pers.iloc[student_idx]\n",
    "            plagiarism_type = classify_plagiarism(sim_score)\n",
    "            \n",
    "            similarities.append({\n",
    "                'filename': student_info['filename'],\n",
    "                'group': student_info['groups'],\n",
    "                'person': student_info['persons'],\n",
    "                'similarity': sim_score,\n",
    "                'type': plagiarism_type\n",
    "            })\n",
    "        \n",
    "        # Trier par similarit√© d√©croissante\n",
    "        similarities.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "        \n",
    "        # Afficher les r√©sultats\n",
    "        for sim in similarities:\n",
    "            print(f\"  {sim['filename']:<30} | Sim: {sim['similarity']:.3f} | Type: {sim['type']}\")\n",
    "        \n",
    "        # Statistiques\n",
    "        if similarities:\n",
    "            avg_sim = np.mean([s['similarity'] for s in similarities])\n",
    "            print(f\"\\n  üìä Similarit√© moyenne: {avg_sim:.3f}\")\n",
    "    \n",
    "    elif not original_docs:\n",
    "        print(\"  ‚ö†Ô∏è Aucun document original dans ce cluster\")\n",
    "        print(f\"  üë• {len(student_docs)} documents √©tudiants sans r√©f√©rence originale\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8e1659",
   "metadata": {},
   "source": [
    "# **üéØ 5. Exercice 2: Comparaison de diff√©rents algorithmes de clustering**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85046a26",
   "metadata": {},
   "source": [
    "## **5.1. M√©thodes de vectorisation alternatives**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae8b4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectorizations():\n",
    "    \"\"\"Calcule diff√©rentes repr√©sentations vectorielles du corpus\"\"\"\n",
    "    corpus = pd.concat([tasks_pers[\"resp\"], tasks_orig[\"resp\"]], ignore_index=True)\n",
    "    corpus_preprocessed = [preprocess_text(doc) for doc in corpus]\n",
    "    \n",
    "    vectorizations = {}\n",
    "    \n",
    "    # 1. One-Hot Vector (OHV)\n",
    "    print(\"‚è≥ Vectorisation OHV...\")\n",
    "    ohv_vectorizer = CountVectorizer(binary=True, max_features=1000)\n",
    "    vectorizations['OHV'] = ohv_vectorizer.fit_transform(corpus_preprocessed)\n",
    "    \n",
    "    # 2. Bag of Words (BOW)\n",
    "    print(\"‚è≥ Vectorisation BOW...\")\n",
    "    bow_vectorizer = CountVectorizer(max_features=1000)\n",
    "    vectorizations['BOW'] = bow_vectorizer.fit_transform(corpus_preprocessed)\n",
    "    \n",
    "    # 3. TF-IDF (d√©j√† calcul√© mais on le refait pour coh√©rence)\n",
    "    print(\"‚è≥ Vectorisation TF-IDF...\")\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=1000, min_df=2, max_df=0.8)\n",
    "    vectorizations['TFIDF'] = tfidf_vectorizer.fit_transform(corpus_preprocessed)\n",
    "    \n",
    "    # 4. Word2Vec (moyenne des embeddings de mots)\n",
    "    try:\n",
    "        from gensim.models import Word2Vec\n",
    "        print(\"‚è≥ Vectorisation Word2Vec...\")\n",
    "        \n",
    "        tokenized_docs = [doc.split() for doc in corpus_preprocessed]\n",
    "        w2v_model = Word2Vec(sentences=tokenized_docs, vector_size=100, window=5, min_count=2, workers=4)\n",
    "        \n",
    "        def doc_to_vec(doc):\n",
    "            words = [w for w in doc.split() if w in w2v_model.wv.index_to_key]\n",
    "            if len(words) > 0:\n",
    "                return np.mean(w2v_model.wv[words], axis=0)\n",
    "            else:\n",
    "                return np.zeros(100)\n",
    "        \n",
    "        w2v_vectors = np.array([doc_to_vec(doc) for doc in corpus_preprocessed])\n",
    "        from scipy.sparse import csr_matrix\n",
    "        vectorizations['Word2Vec'] = csr_matrix(w2v_vectors)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Word2Vec non disponible: {e}\")\n",
    "    \n",
    "    # 5. Doc2Vec\n",
    "    try:\n",
    "        from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "        print(\"‚è≥ Vectorisation Doc2Vec...\")\n",
    "        \n",
    "        tagged_docs = [TaggedDocument(words=doc.split(), tags=[str(i)]) \n",
    "                       for i, doc in enumerate(corpus_preprocessed)]\n",
    "        d2v_model = Doc2Vec(tagged_docs, vector_size=100, window=5, min_count=2, workers=4, epochs=20)\n",
    "        \n",
    "        d2v_vectors = np.array([d2v_model.infer_vector(doc.split()) for doc in corpus_preprocessed])\n",
    "        vectorizations['Doc2Vec'] = csr_matrix(d2v_vectors)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Doc2Vec non disponible: {e}\")\n",
    "    \n",
    "    print(f\"‚úÖ {len(vectorizations)} m√©thodes de vectorisation calcul√©es\")\n",
    "    return vectorizations\n",
    "\n",
    "# Calcul des diff√©rentes vectorisations\n",
    "vectorizations = get_vectorizations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0ef25d",
   "metadata": {},
   "source": [
    "## **5.2. Algorithmes de clustering alternatifs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586198de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_clustering_algorithms(vectors_dict):\n",
    "    \"\"\"Applique diff√©rents algorithmes de clustering sur diff√©rentes vectorisations\"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for vec_name, vectors in vectors_dict.items():\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Vectorisation: {vec_name}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # R√©duction de dimensionnalit√© pour tous les algos\n",
    "        pca = PCA(n_components=min(50, vectors.shape[1]), random_state=42)\n",
    "        vectors_reduced = pca.fit_transform(vectors.toarray())\n",
    "        \n",
    "        # PCA 2D pour visualisation\n",
    "        pca_2d = PCA(n_components=2, random_state=42)\n",
    "        vectors_2d = pca_2d.fit_transform(vectors.toarray())\n",
    "        \n",
    "        results[vec_name] = {}\n",
    "        \n",
    "        # 1. K-Means\n",
    "        try:\n",
    "            print(f\"  ‚è≥ K-Means...\")\n",
    "            kmeans = KMeans(n_clusters=5, init='k-means++', n_init=20, max_iter=300, random_state=42)\n",
    "            labels_km = kmeans.fit_predict(vectors_2d)\n",
    "            \n",
    "            results[vec_name]['KMeans'] = {\n",
    "                'labels': labels_km,\n",
    "                'silhouette': silhouette_score(vectors_2d, labels_km),\n",
    "                'calinski': calinski_harabasz_score(vectors_2d, labels_km),\n",
    "                'davies_bouldin': davies_bouldin_score(vectors_2d, labels_km),\n",
    "                'vectors_2d': vectors_2d\n",
    "            }\n",
    "            print(f\"     ‚úÖ Silhouette: {results[vec_name]['KMeans']['silhouette']:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"     ‚ùå Erreur: {e}\")\n",
    "        \n",
    "        # 2. MiniBatch K-Means (plus rapide pour grands datasets)\n",
    "        try:\n",
    "            from sklearn.cluster import MiniBatchKMeans\n",
    "            print(f\"  ‚è≥ MiniBatch K-Means...\")\n",
    "            mb_kmeans = MiniBatchKMeans(n_clusters=5, random_state=42, batch_size=100)\n",
    "            labels_mbkm = mb_kmeans.fit_predict(vectors_2d)\n",
    "            \n",
    "            results[vec_name]['MiniBatchKMeans'] = {\n",
    "                'labels': labels_mbkm,\n",
    "                'silhouette': silhouette_score(vectors_2d, labels_mbkm),\n",
    "                'calinski': calinski_harabasz_score(vectors_2d, labels_mbkm),\n",
    "                'davies_bouldin': davies_bouldin_score(vectors_2d, labels_mbkm),\n",
    "                'vectors_2d': vectors_2d\n",
    "            }\n",
    "            print(f\"     ‚úÖ Silhouette: {results[vec_name]['MiniBatchKMeans']['silhouette']:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"     ‚ùå Erreur: {e}\")\n",
    "        \n",
    "        # 3. Spectral Clustering\n",
    "        try:\n",
    "            print(f\"  ‚è≥ Spectral Clustering...\")\n",
    "            spectral = SpectralClustering(n_clusters=5, random_state=42, affinity='nearest_neighbors')\n",
    "            labels_spectral = spectral.fit_predict(vectors_reduced)\n",
    "            \n",
    "            results[vec_name]['SpectralClustering'] = {\n",
    "                'labels': labels_spectral,\n",
    "                'silhouette': silhouette_score(vectors_reduced, labels_spectral),\n",
    "                'calinski': calinski_harabasz_score(vectors_reduced, labels_spectral),\n",
    "                'davies_bouldin': davies_bouldin_score(vectors_reduced, labels_spectral),\n",
    "                'vectors_2d': vectors_2d\n",
    "            }\n",
    "            print(f\"     ‚úÖ Silhouette: {results[vec_name]['SpectralClustering']['silhouette']:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"     ‚ùå Erreur: {e}\")\n",
    "        \n",
    "        # 4. DBSCAN\n",
    "        try:\n",
    "            print(f\"  ‚è≥ DBSCAN...\")\n",
    "            dbscan = DBSCAN(eps=0.5, min_samples=3)\n",
    "            labels_dbscan = dbscan.fit_predict(vectors_reduced)\n",
    "            \n",
    "            n_clusters = len(set(labels_dbscan)) - (1 if -1 in labels_dbscan else 0)\n",
    "            n_noise = list(labels_dbscan).count(-1)\n",
    "            \n",
    "            if n_clusters > 1:\n",
    "                results[vec_name]['DBSCAN'] = {\n",
    "                    'labels': labels_dbscan,\n",
    "                    'silhouette': silhouette_score(vectors_reduced, labels_dbscan) if n_clusters > 1 else 0,\n",
    "                    'n_clusters': n_clusters,\n",
    "                    'n_noise': n_noise,\n",
    "                    'vectors_2d': vectors_2d\n",
    "                }\n",
    "                print(f\"     ‚úÖ Clusters trouv√©s: {n_clusters}, Bruit: {n_noise}\")\n",
    "            else:\n",
    "                print(f\"     ‚ö†Ô∏è Pas assez de clusters d√©tect√©s: {n_clusters}\")\n",
    "        except Exception as e:\n",
    "            print(f\"     ‚ùå Erreur: {e}\")\n",
    "        \n",
    "        # 5. OPTICS\n",
    "        try:\n",
    "            print(f\"  ‚è≥ OPTICS...\")\n",
    "            optics = OPTICS(min_samples=3, max_eps=2.0)\n",
    "            labels_optics = optics.fit_predict(vectors_reduced)\n",
    "            \n",
    "            n_clusters = len(set(labels_optics)) - (1 if -1 in labels_optics else 0)\n",
    "            \n",
    "            if n_clusters > 1:\n",
    "                results[vec_name]['OPTICS'] = {\n",
    "                    'labels': labels_optics,\n",
    "                    'silhouette': silhouette_score(vectors_reduced, labels_optics) if n_clusters > 1 else 0,\n",
    "                    'n_clusters': n_clusters,\n",
    "                    'vectors_2d': vectors_2d\n",
    "                }\n",
    "                print(f\"     ‚úÖ Clusters trouv√©s: {n_clusters}\")\n",
    "            else:\n",
    "                print(f\"     ‚ö†Ô∏è Pas assez de clusters d√©tect√©s: {n_clusters}\")\n",
    "        except Exception as e:\n",
    "            print(f\"     ‚ùå Erreur: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Application des algorithmes\n",
    "clustering_results = apply_clustering_algorithms(vectorizations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed2c24c",
   "metadata": {},
   "source": [
    "## **5.3. Comparaison et visualisation des r√©sultats**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262a49e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©ation du tableau comparatif\n",
    "comparison_data = []\n",
    "\n",
    "for vec_name, algo_results in clustering_results.items():\n",
    "    for algo_name, metrics in algo_results.items():\n",
    "        if 'silhouette' in metrics:\n",
    "            comparison_data.append({\n",
    "                'Vectorisation': vec_name,\n",
    "                'Algorithme': algo_name,\n",
    "                'Silhouette': metrics['silhouette'],\n",
    "                'Calinski-Harabasz': metrics.get('calinski', 'N/A'),\n",
    "                'Davies-Bouldin': metrics.get('davies_bouldin', 'N/A'),\n",
    "                'N_Clusters': metrics.get('n_clusters', 5)\n",
    "            })\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"üìä TABLEAU COMPARATIF DES PERFORMANCES\")\n",
    "print(\"=\"*100)\n",
    "print(df_comparison.to_string(index=False))\n",
    "\n",
    "# Identification de la meilleure combinaison\n",
    "best_result = df_comparison.loc[df_comparison['Silhouette'].idxmax()]\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"üèÜ MEILLEURE COMBINAISON\")\n",
    "print(\"=\"*100)\n",
    "print(f\"Vectorisation: {best_result['Vectorisation']}\")\n",
    "print(f\"Algorithme: {best_result['Algorithme']}\")\n",
    "print(f\"Silhouette Score: {best_result['Silhouette']:.4f}\")\n",
    "print(f\"Calinski-Harabasz Score: {best_result['Calinski-Harabasz']}\")\n",
    "print(f\"Davies-Bouldin Score: {best_result['Davies-Bouldin']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027e92c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation comparative avec graphiques\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Comparaison des Silhouette Scores\n",
    "ax1 = axes[0, 0]\n",
    "df_pivot = df_comparison.pivot(index='Algorithme', columns='Vectorisation', values='Silhouette')\n",
    "df_pivot.plot(kind='bar', ax=ax1, colormap='viridis')\n",
    "ax1.set_title('Silhouette Score par Algorithme et Vectorisation', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('Silhouette Score', fontsize=12)\n",
    "ax1.set_xlabel('Algorithme', fontsize=12)\n",
    "ax1.legend(title='Vectorisation', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# 2. Heatmap des Silhouette Scores\n",
    "ax2 = axes[0, 1]\n",
    "sns.heatmap(df_pivot, annot=True, fmt='.3f', cmap='YlGnBu', ax=ax2, cbar_kws={'label': 'Silhouette'})\n",
    "ax2.set_title('Heatmap des Silhouette Scores', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Vectorisation', fontsize=12)\n",
    "ax2.set_ylabel('Algorithme', fontsize=12)\n",
    "\n",
    "# 3. Top 5 combinaisons\n",
    "ax3 = axes[1, 0]\n",
    "top_5 = df_comparison.nlargest(5, 'Silhouette')\n",
    "top_5['Combinaison'] = top_5['Vectorisation'] + '\\n+ ' + top_5['Algorithme']\n",
    "ax3.barh(range(len(top_5)), top_5['Silhouette'], color='steelblue')\n",
    "ax3.set_yticks(range(len(top_5)))\n",
    "ax3.set_yticklabels(top_5['Combinaison'], fontsize=10)\n",
    "ax3.set_xlabel('Silhouette Score', fontsize=12)\n",
    "ax3.set_title('Top 5 Meilleures Combinaisons', fontsize=14, fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3, axis='x')\n",
    "ax3.invert_yaxis()\n",
    "\n",
    "# 4. Distribution des scores par vectorisation\n",
    "ax4 = axes[1, 1]\n",
    "df_comparison.boxplot(column='Silhouette', by='Vectorisation', ax=ax4)\n",
    "ax4.set_title('Distribution des Silhouette Scores par Vectorisation', fontsize=14, fontweight='bold')\n",
    "ax4.set_xlabel('Vectorisation', fontsize=12)\n",
    "ax4.set_ylabel('Silhouette Score', fontsize=12)\n",
    "plt.setp(ax4.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "plt.suptitle('')  # Supprimer le titre automatique de boxplot\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d13320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des clusters pour les meilleures m√©thodes\n",
    "best_vec = best_result['Vectorisation']\n",
    "best_algo = best_result['Algorithme']\n",
    "\n",
    "print(f\"\\nüìç Visualisation d√©taill√©e: {best_vec} + {best_algo}\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "plot_idx = 0\n",
    "for vec_name in list(clustering_results.keys())[:3]:  # Top 3 vectorisations\n",
    "    for algo_name in ['KMeans', 'SpectralClustering']:\n",
    "        if algo_name in clustering_results[vec_name]:\n",
    "            ax = axes[plot_idx]\n",
    "            \n",
    "            result = clustering_results[vec_name][algo_name]\n",
    "            vectors_2d = result['vectors_2d']\n",
    "            labels = result['labels']\n",
    "            \n",
    "            scatter = ax.scatter(vectors_2d[:, 0], vectors_2d[:, 1], \n",
    "                               c=labels, s=30, cmap='viridis', alpha=0.6)\n",
    "            \n",
    "            ax.set_title(f'{vec_name} + {algo_name}\\nSilhouette: {result[\"silhouette\"]:.4f}',\n",
    "                        fontsize=11, fontweight='bold')\n",
    "            ax.set_xlabel('PC1', fontsize=10)\n",
    "            ax.set_ylabel('PC2', fontsize=10)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            plot_idx += 1\n",
    "            if plot_idx >= 6:\n",
    "                break\n",
    "    if plot_idx >= 6:\n",
    "        break\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdcd219",
   "metadata": {},
   "source": [
    "# **üìù 6. Analyse d√©taill√©e et interpr√©tation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584359ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse de la qualit√© de d√©tection du plagiat pour la meilleure m√©thode\n",
    "print(\"=\"*100)\n",
    "print(\"üîç ANALYSE D√âTAILL√âE DE LA D√âTECTION DE PLAGIAT\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "best_labels = clustering_results[best_vec][best_algo]['labels']\n",
    "\n",
    "# Cr√©er un dictionnaire des clusters\n",
    "best_clusters = {}\n",
    "for i in range(max(best_labels) + 1):\n",
    "    cluster_indices = [idx for idx, x in enumerate(best_labels) if x == i]\n",
    "    best_clusters[f'Cluster_{i}'] = cluster_indices\n",
    "\n",
    "# Analyse par t√¢che originale\n",
    "num_student_docs = len(tasks_pers)\n",
    "tasks_list = sorted(tasks_orig['tasks'].unique())\n",
    "\n",
    "print(f\"\\nüìã Analyse par t√¢che originale (m√©thode: {best_vec} + {best_algo}):\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "detection_stats = {\n",
    "    'Cut': 0,\n",
    "    'Heavy': 0,\n",
    "    'Light': 0,\n",
    "    'Non': 0\n",
    "}\n",
    "\n",
    "for task in tasks_list:\n",
    "    print(f\"\\nüéØ T√¢che {task.upper()}:\")\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    # Trouver l'index du document original\n",
    "    orig_idx = tasks_orig[tasks_orig['tasks'] == task].index[0] + num_student_docs\n",
    "    \n",
    "    # Trouver le cluster contenant l'original\n",
    "    orig_cluster = best_labels[orig_idx]\n",
    "    cluster_docs = [idx for idx, lbl in enumerate(best_labels) if lbl == orig_cluster]\n",
    "    \n",
    "    # Documents √©tudiants dans ce cluster\n",
    "    student_docs_in_cluster = [idx for idx in cluster_docs if idx < num_student_docs]\n",
    "    student_docs_task = tasks_pers[tasks_pers['tasks'] == task].index.tolist()\n",
    "    \n",
    "    # Calcul de similarit√© avec l'original\n",
    "    similarities = []\n",
    "    for student_idx in student_docs_task[:15]:  # Limiter √† 15\n",
    "        sim_score = similarity_matrix[orig_idx][student_idx]\n",
    "        plagiarism_type = classify_plagiarism(sim_score)\n",
    "        detection_stats[plagiarism_type.split()[0]] += 1\n",
    "        \n",
    "        in_same_cluster = \"‚úÖ\" if student_idx in student_docs_in_cluster else \"‚ùå\"\n",
    "        \n",
    "        student_info = tasks_pers.iloc[student_idx]\n",
    "        similarities.append({\n",
    "            'group': student_info['groups'],\n",
    "            'person': student_info['persons'],\n",
    "            'similarity': sim_score,\n",
    "            'type': plagiarism_type,\n",
    "            'clustered': in_same_cluster\n",
    "        })\n",
    "    \n",
    "    # Trier par similarit√©\n",
    "    similarities.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "    \n",
    "    # Afficher top 10\n",
    "    print(f\"  üìä Documents √©tudiants analys√©s: {len(similarities)}\")\n",
    "    print(f\"  üìç Documents dans le m√™me cluster que l'original: {len(student_docs_in_cluster)}\")\n",
    "    print(f\"\\n  Top 10 documents par similarit√©:\")\n",
    "    print(f\"  {'Groupe':<8} {'Pers.':<6} {'Similarit√©':<12} {'Type':<20} {'Cluster'}\")\n",
    "    print(f\"  {'-'*70}\")\n",
    "    \n",
    "    for sim in similarities[:10]:\n",
    "        print(f\"  {sim['group']:<8} {sim['person']:<6} {sim['similarity']:<12.3f} \"\n",
    "              f\"{sim['type']:<20} {sim['clustered']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"üìä STATISTIQUES GLOBALES DE D√âTECTION\")\n",
    "print(\"=\"*100)\n",
    "print(f\"Total de documents analys√©s: {sum(detection_stats.values())}\")\n",
    "for plag_type, count in detection_stats.items():\n",
    "    percentage = (count / sum(detection_stats.values())) * 100 if sum(detection_stats.values()) > 0 else 0\n",
    "    print(f\"  - {plag_type:<10}: {count:>3} documents ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a8658c",
   "metadata": {},
   "source": [
    "# **üìä 7. Conclusion et recommandations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4dd6b5",
   "metadata": {},
   "source": [
    "## **7.1. Synth√®se des r√©sultats**\n",
    "\n",
    "### **üéØ Performances des m√©thodes de vectorisation:**\n",
    "\n",
    "1. **TF-IDF** : M√©thode de r√©f√©rence\n",
    "   - ‚úÖ Excellente performance pour la d√©tection de similarit√© textuelle\n",
    "   - ‚úÖ Capture bien les mots-cl√©s importants\n",
    "   - ‚úÖ Peu sensible au bruit\n",
    "   - ‚ö†Ô∏è Ne capture pas les relations s√©mantiques\n",
    "\n",
    "2. **BOW (Bag of Words)** : Performance similaire √† TF-IDF\n",
    "   - ‚úÖ Simple et efficace\n",
    "   - ‚ö†Ô∏è Sensible aux mots fr√©quents\n",
    "   - ‚ö†Ô∏è Pas de pond√©ration par importance\n",
    "\n",
    "3. **OHV (One-Hot Vector)** : Performance mod√©r√©e\n",
    "   - ‚úÖ Tr√®s simple √† impl√©menter\n",
    "   - ‚ùå Perd l'information de fr√©quence\n",
    "   - ‚ùå Moins discriminant\n",
    "\n",
    "4. **Word2Vec / Doc2Vec** : Performance variable\n",
    "   - ‚úÖ Capture les relations s√©mantiques\n",
    "   - ‚úÖ D√©tecte les paraphrases\n",
    "   - ‚ö†Ô∏è N√©cessite beaucoup de donn√©es d'entra√Ænement\n",
    "   - ‚ö†Ô∏è Plus lent √† calculer\n",
    "\n",
    "### **üéØ Performances des algorithmes de clustering:**\n",
    "\n",
    "1. **K-Means** : Algorithme de r√©f√©rence\n",
    "   - ‚úÖ Rapide et efficace\n",
    "   - ‚úÖ R√©sultats consistants\n",
    "   - ‚úÖ Bon pour des clusters sph√©riques\n",
    "   - ‚ö†Ô∏è N√©cessite de sp√©cifier k √† l'avance\n",
    "\n",
    "2. **MiniBatch K-Means** : Alternative rapide\n",
    "   - ‚úÖ Tr√®s rapide pour grands datasets\n",
    "   - ‚úÖ R√©sultats similaires √† K-Means\n",
    "   - ‚ö†Ô∏è L√©g√®rement moins pr√©cis\n",
    "\n",
    "3. **Spectral Clustering** : Bon pour clusters complexes\n",
    "   - ‚úÖ D√©tecte des formes non-sph√©riques\n",
    "   - ‚úÖ Bonne s√©paration des groupes\n",
    "   - ‚ùå Lent sur grands datasets\n",
    "   - ‚ùå Sensible aux param√®tres\n",
    "\n",
    "4. **DBSCAN / OPTICS** : D√©tection de densit√©\n",
    "   - ‚úÖ Pas besoin de sp√©cifier k\n",
    "   - ‚úÖ D√©tecte les outliers\n",
    "   - ‚ùå Sensible aux param√®tres (eps, min_samples)\n",
    "   - ‚ùå Performance variable sur ce corpus\n",
    "\n",
    "### **üèÜ Meilleure combinaison identifi√©e:**\n",
    "\n",
    "La meilleure combinaison varie selon les donn√©es, mais g√©n√©ralement:\n",
    "- **TF-IDF + K-Means** : Meilleur compromis performance/vitesse\n",
    "- **TF-IDF + Spectral Clustering** : Meilleure s√©paration des groupes\n",
    "- **Doc2Vec + K-Means** : Meilleure d√©tection s√©mantique (si corpus suffisant)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec6a4a5",
   "metadata": {},
   "source": [
    "## **7.2. Interpr√©tation des types de plagiat**\n",
    "\n",
    "### **Classification selon la similarit√© cosinus:**\n",
    "\n",
    "| Type de Plagiat | Similarit√© | Caract√©ristiques | D√©tection |\n",
    "|-----------------|------------|------------------|-----------|\n",
    "| **Cut** | 70-100% | Copier-coller direct du texte original | ‚úÖ Tr√®s facile |\n",
    "| **Heavy** | 40-70% | Copier-coller avec reformulations importantes | ‚úÖ Facile |\n",
    "| **Light** | 10-40% | Quelques extraits avec paraphrases | ‚ö†Ô∏è Mod√©r√© |\n",
    "| **Non** | <10% | Pas de copie, texte original | ‚ùå Difficile |\n",
    "\n",
    "### **Observations:**\n",
    "\n",
    "1. **Documents \"Cut\" (Plagiat clair):**\n",
    "   - Tr√®s forte similarit√© avec l'original (>70%)\n",
    "   - Facilement regroup√©s dans le m√™me cluster\n",
    "   - Structure et mots-cl√©s identiques\n",
    "\n",
    "2. **Documents \"Heavy\":**\n",
    "   - Bonne similarit√© (40-70%)\n",
    "   - Souvent dans le m√™me cluster ou cluster adjacent\n",
    "   - Mots-cl√©s similaires mais ordre diff√©rent\n",
    "\n",
    "3. **Documents \"Light\":**\n",
    "   - Similarit√© mod√©r√©e (10-40%)\n",
    "   - Peuvent √™tre dans des clusters diff√©rents\n",
    "   - Reformulations importantes, vocabulaire vari√©\n",
    "\n",
    "4. **Documents \"Non\":**\n",
    "   - Faible similarit√© (<10%)\n",
    "   - G√©n√©ralement dans des clusters diff√©rents\n",
    "   - Texte original, seulement le sujet en commun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9400f7de",
   "metadata": {},
   "source": [
    "## **7.3. Recommandations pour la d√©tection de plagiat**\n",
    "\n",
    "### **üéØ Pour un syst√®me de d√©tection optimal:**\n",
    "\n",
    "1. **Approche hybride recommand√©e:**\n",
    "   ```\n",
    "   TF-IDF (similarit√© lexicale) + Doc2Vec (similarit√© s√©mantique)\n",
    "   ```\n",
    "   - Combine les avantages des deux approches\n",
    "   - D√©tecte √† la fois les copies exactes et les paraphrases\n",
    "\n",
    "2. **Processus de d√©tection en deux √©tapes:**\n",
    "   - **√âtape 1:** Clustering pour regrouper les documents similaires\n",
    "   - **√âtape 2:** Calcul de similarit√© fine dans chaque cluster\n",
    "\n",
    "3. **Seuils de d√©tection adaptatifs:**\n",
    "   - Ajuster les seuils selon le contexte (acad√©mique, professionnel)\n",
    "   - Consid√©rer le niveau d'√©tudes (lyc√©e, universit√©, doctorat)\n",
    "\n",
    "4. **Pr√©traitement crucial:**\n",
    "   - Lemmatization pour normaliser les variations\n",
    "   - Suppression des stopwords pour se concentrer sur le contenu\n",
    "   - Gestion des citations et r√©f√©rences\n",
    "\n",
    "### **‚ö†Ô∏è Limites et pr√©cautions:**\n",
    "\n",
    "1. **Faux positifs possibles:**\n",
    "   - Documents traitant du m√™me sujet peuvent √™tre similaires\n",
    "   - Terminologie technique commune peut augmenter la similarit√©\n",
    "\n",
    "2. **Faux n√©gatifs possibles:**\n",
    "   - Paraphrases sophistiqu√©es peuvent √©chapper √† la d√©tection\n",
    "   - Traductions ou reformulations cr√©atives\n",
    "\n",
    "3. **Consid√©rations √©thiques:**\n",
    "   - Ne pas se fier uniquement aux m√©triques automatiques\n",
    "   - Toujours v√©rifier manuellement les cas suspects\n",
    "   - Respecter le droit √† la d√©fense des √©tudiants\n",
    "\n",
    "### **üöÄ Am√©liorations futures:**\n",
    "\n",
    "1. **Utilisation de mod√®les de langage modernes:**\n",
    "   - BERT, GPT pour la compr√©hension s√©mantique\n",
    "   - Transformers pour la d√©tection contextuelle\n",
    "\n",
    "2. **Analyse multi-documents:**\n",
    "   - D√©tection de plagiat √† partir de sources multiples\n",
    "   - Identification de la source originale\n",
    "\n",
    "3. **Analyse temporelle:**\n",
    "   - Historique des soumissions\n",
    "   - D√©tection de l'√©volution du style d'√©criture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae71bf74",
   "metadata": {},
   "source": [
    "## **7.4. Conclusion finale**\n",
    "\n",
    "Ce travail sur la d√©tection de plagiat par clustering a permis de:\n",
    "\n",
    "### **‚úÖ R√©sultats obtenus:**\n",
    "\n",
    "1. **√âvaluation comparative compl√®te:**\n",
    "   - Test√© 5+ m√©thodes de vectorisation (OHV, BOW, TF-IDF, Word2Vec, Doc2Vec)\n",
    "   - Appliqu√© 5+ algorithmes de clustering (K-Means, MiniBatch K-Means, Spectral, DBSCAN, OPTICS)\n",
    "   - Identifi√© les meilleures combinaisons pour la d√©tection de plagiat\n",
    "\n",
    "2. **Syst√®me de classification des types de plagiat:**\n",
    "   - Cut (70-100%) : Copie directe\n",
    "   - Heavy (40-70%) : Copie avec reformulations\n",
    "   - Light (10-40%) : Paraphrases l√©g√®res\n",
    "   - Non (<10%) : Texte original\n",
    "\n",
    "3. **M√©triques d'√©valuation:**\n",
    "   - Silhouette Score : Qualit√© de la s√©paration des clusters\n",
    "   - Calinski-Harabasz : Densit√© et s√©paration\n",
    "   - Davies-Bouldin : Compacit√© des clusters\n",
    "   - Similarit√© cosinus : Mesure directe de plagiat\n",
    "\n",
    "### **üéì Apprentissages cl√©s:**\n",
    "\n",
    "1. **TF-IDF reste la m√©thode de r√©f√©rence** pour ce type de t√¢che\n",
    "2. **K-Means offre le meilleur compromis** entre performance et rapidit√©\n",
    "3. **Le pr√©traitement est crucial** pour la qualit√© des r√©sultats\n",
    "4. **Approche hybride recommand√©e** pour une d√©tection robuste\n",
    "\n",
    "### **üí° Applications pratiques:**\n",
    "\n",
    "- Syst√®mes de d√©tection de plagiat acad√©mique\n",
    "- V√©rification d'originalit√© de contenus\n",
    "- D√©tection de duplication dans les bases documentaires\n",
    "- Analyse de similarit√© de textes juridiques ou techniques\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
