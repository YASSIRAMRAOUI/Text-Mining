{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMKs/ulzfbEHkrSRuVnf5+s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YASSIRAMRAOUI/Text-Mining/blob/main/Atelier_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Traduction assistée en NLTK***"
      ],
      "metadata": {
        "id": "2Fy_7Vq25gV9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Étape 1 : Préparation de l’environnement**"
      ],
      "metadata": {
        "id": "G2W7IfvzPEPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Téléchargements nécessaires\n",
        "nltk.download(['punkt', 'averaged_perceptron_tagger', 'wordnet', 'omw-1.4', 'stopwords', 'punkt_tab'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0RB0CH2PMVy",
        "outputId": "301759a5-846b-42a2-dc45-b160b71065dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Étape 2 : Saisie utilisateur**"
      ],
      "metadata": {
        "id": "-hXf0ilFPf0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = input(\"Veuillez entrer un paragraphe en anglais (minimum 3 phrases) :\\n> \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMW2waEcPoj_",
        "outputId": "48c46b7d-a2cf-4984-d10a-bd3a345ed408"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Veuillez entrer un paragraphe en anglais (minimum 3 phrases) :\n",
            "> I took a short break after finishing the report.  The company plans to increase its profits.  She is reading an interesting book about history.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Étape 3 : Segmentation en phrases**"
      ],
      "metadata": {
        "id": "Q9f2cyV4P1em"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "sentences = sent_tokenize(text) # On découpe le texte en phrases.\n",
        "print(\"Phrases détectées :\")\n",
        "for s in sentences:\n",
        "    print(\"-\", s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z52v24Y2QIKF",
        "outputId": "7601930b-b80e-48fd-ddb4-b033ca05485c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phrases détectées :\n",
            "- I took a short break after finishing the report.\n",
            "- The company plans to increase its profits.\n",
            "- She is reading an interesting book about history.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Étape 4 : Tokenisation + POS tagging**"
      ],
      "metadata": {
        "id": "0L0GDxAORVLI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "all_pos = []\n",
        "for sent in sentences:\n",
        "    tokens = word_tokenize(sent) # On découpe chaque phrase en mots (tokens).\n",
        "    tagged = pos_tag(tokens) # On applique le POS tagging (catégorie grammaticale).\n",
        "    all_pos.append(tagged)\n",
        "    print(\"\\nPhrase :\", sent)\n",
        "    print(tagged)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjyDswxSRnvg",
        "outputId": "8d4314f7-65f8-498f-843c-b57acecc048f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Phrase : I took a short break after finishing the report.\n",
            "[('I', 'PRP'), ('took', 'VBD'), ('a', 'DT'), ('short', 'JJ'), ('break', 'NN'), ('after', 'IN'), ('finishing', 'VBG'), ('the', 'DT'), ('report', 'NN'), ('.', '.')]\n",
            "\n",
            "Phrase : The company plans to increase its profits.\n",
            "[('The', 'DT'), ('company', 'NN'), ('plans', 'VBZ'), ('to', 'TO'), ('increase', 'VB'), ('its', 'PRP$'), ('profits', 'NNS'), ('.', '.')]\n",
            "\n",
            "Phrase : She is reading an interesting book about history.\n",
            "[('She', 'PRP'), ('is', 'VBZ'), ('reading', 'VBG'), ('an', 'DT'), ('interesting', 'JJ'), ('book', 'NN'), ('about', 'IN'), ('history', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Étape 5 : Filtrage des mots significatifs**"
      ],
      "metadata": {
        "id": "npGPI5UBSPP4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('N'):\n",
        "        return nltk.corpus.wordnet.NOUN\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return nltk.corpus.wordnet.VERB\n",
        "    elif treebank_tag.startswith('J'):\n",
        "        return nltk.corpus.wordnet.ADJ\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "keywords_per_sentence = []\n",
        "for tagged_sent in all_pos:\n",
        "    keywords = [(w, get_wordnet_pos(p)) for w, p in tagged_sent if get_wordnet_pos(p)]\n",
        "    keywords_per_sentence.append(keywords) # On garde seulement Noms, Verbes, Adjectifs.\n",
        "\n",
        "print(\"Mots significatifs par phrase :\")\n",
        "for sent, kws in zip(sentences, keywords_per_sentence):\n",
        "    print(\"\\n-\", sent)\n",
        "    print(kws)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaWdLdjhSX38",
        "outputId": "3dc88b09-e747-4009-bea8-4e2c5f0fb29f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mots significatifs par phrase :\n",
            "\n",
            "- I took a short break after finishing the report.\n",
            "[('took', 'v'), ('short', 'a'), ('break', 'n'), ('finishing', 'v'), ('report', 'n')]\n",
            "\n",
            "- The company plans to increase its profits.\n",
            "[('company', 'n'), ('plans', 'v'), ('increase', 'v'), ('profits', 'n')]\n",
            "\n",
            "- She is reading an interesting book about history.\n",
            "[('is', 'v'), ('reading', 'v'), ('interesting', 'a'), ('book', 'n'), ('history', 'n')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Étape 6 : Désambiguïsation avec Lesk**"
      ],
      "metadata": {
        "id": "GdWIqK3PTPZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.wsd import lesk\n",
        "\n",
        "results = [] # On applique l’algorithme Lesk de NLTK pour trouver le bon sens selon le contexte.\n",
        "for sent, kws in zip(sentences, keywords_per_sentence):\n",
        "    tokens = word_tokenize(sent)\n",
        "    word_info = []\n",
        "    for word, wn_pos in kws:\n",
        "        synset = lesk(tokens, word, wn_pos)\n",
        "        if synset:\n",
        "            definition = synset.definition()\n",
        "            translations = [lemma.name() for lemma in synset.lemmas(lang='fra')]\n",
        "            if not translations:\n",
        "                translations = [\"(pas de traduction disponible)\"]\n",
        "            word_info.append({\n",
        "                \"word\": word,\n",
        "                \"pos\": wn_pos,\n",
        "                \"definition\": definition,\n",
        "                \"translations\": translations\n",
        "            })\n",
        "    results.append({\"sentence\": sent, \"keywords\": word_info})"
      ],
      "metadata": {
        "id": "lqDjjlStTTU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Étape 7 : Affichage final**"
      ],
      "metadata": {
        "id": "DRR2Sik0T47d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== Assistant de traduction ===\\n\")\n",
        "for res in results:\n",
        "    print(\"Phrase :\", res[\"sentence\"])\n",
        "    for kw in res[\"keywords\"]:\n",
        "        print(f\" - Mot : {kw['word']}\")\n",
        "        print(f\"   > Sens (EN) : {kw['definition']}\") # On propose à l’utilisateur le sens\n",
        "        print(f\"   > Traduction(s) FR : {', '.join(kw['translations'])}\") # Traduction possible en français\n",
        "    print(\"-\" * 60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWcTYQrDT9tk",
        "outputId": "9ae4c957-286b-4aef-8cde-544831547016"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Assistant de traduction ===\n",
            "\n",
            "Phrase : I took a short break after finishing the report.\n",
            " - Mot : took\n",
            "   > Sens (EN) : develop a habit\n",
            "   > Traduction(s) FR : (pas de traduction disponible)\n",
            " - Mot : short\n",
            "   > Sens (EN) : of speech sounds or syllables of relatively short duration\n",
            "   > Traduction(s) FR : court\n",
            " - Mot : break\n",
            "   > Sens (EN) : (geology) a crack in the earth's crust resulting from the displacement of one side with respect to the other\n",
            "   > Traduction(s) FR : casser, cassure, fracture\n",
            " - Mot : finishing\n",
            "   > Sens (EN) : cause to finish a relationship with somebody\n",
            "   > Traduction(s) FR : achever, finir\n",
            " - Mot : report\n",
            "   > Sens (EN) : a short account of the news\n",
            "   > Traduction(s) FR : compte, description, histoire, rapport, renseignement, récit, étage\n",
            "------------------------------------------------------------\n",
            "Phrase : The company plans to increase its profits.\n",
            " - Mot : company\n",
            "   > Sens (EN) : an institution created to conduct business\n",
            "   > Traduction(s) FR : compagnie, entreprise, société\n",
            " - Mot : plans\n",
            "   > Sens (EN) : make plans for something\n",
            "   > Traduction(s) FR : planifier, programmer\n",
            " - Mot : increase\n",
            "   > Sens (EN) : make bigger or more\n",
            "   > Traduction(s) FR : accroire, accroître, augmenter\n",
            " - Mot : profits\n",
            "   > Sens (EN) : something won (especially money)\n",
            "   > Traduction(s) FR : gagner\n",
            "------------------------------------------------------------\n",
            "Phrase : She is reading an interesting book about history.\n",
            " - Mot : is\n",
            "   > Sens (EN) : have an existence, be extant\n",
            "   > Traduction(s) FR : exister, être\n",
            " - Mot : reading\n",
            "   > Sens (EN) : audition for a stage role by reading parts of a role\n",
            "   > Traduction(s) FR : lire\n",
            " - Mot : interesting\n",
            "   > Sens (EN) : arousing or holding the attention\n",
            "   > Traduction(s) FR : attractif, curieux, intéressant, singulier\n",
            " - Mot : book\n",
            "   > Sens (EN) : a written version of a play or other dramatic composition; used in preparing for a performance\n",
            "   > Traduction(s) FR : livre, réserver, scénario\n",
            " - Mot : history\n",
            "   > Sens (EN) : all that is remembered of the past as preserved in writing; a body of knowledge\n",
            "   > Traduction(s) FR : histoire, historique, passé\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Calcul des similarité entre documents texte***"
      ],
      "metadata": {
        "id": "E3Pdl9cU522Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Étape 1 : Installation & imports**"
      ],
      "metadata": {
        "id": "GH7yGH6d56zU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install googletrans==4.0.0-rc1 jaro-winkler\n",
        "\n",
        "import nltk, jaro\n",
        "from googletrans import Translator\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.wsd import lesk\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "# Téléchargement des ressources nécessaires\n",
        "nltk.download(['punkt', 'wordnet', 'omw-1.4'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egD21Z--6Bh-",
        "outputId": "b6eb6dd2-271c-42f2-c663-9b97d6483c04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting googletrans==4.0.0-rc1\n",
            "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jaro-winkler\n",
            "  Downloading jaro_winkler-2.0.3-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2025.8.3)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hstspreload-2025.1.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\n",
            "Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m676.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaro_winkler-2.0.3-py3-none-any.whl (33 kB)\n",
            "Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Downloading hstspreload-2025.1.1-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17396 sha256=f15f3677ff1ee51877009a566327c79995656e996e0145f4fb3f7c42f6401311\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/0f/04/b17a72024b56a60e499ce1a6313d283ed5ba332407155bee03\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, jaro-winkler, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: hyperframe\n",
            "    Found existing installation: hyperframe 6.1.0\n",
            "    Uninstalling hyperframe-6.1.0:\n",
            "      Successfully uninstalled hyperframe-6.1.0\n",
            "  Attempting uninstall: hpack\n",
            "    Found existing installation: hpack 4.1.0\n",
            "    Uninstalling hpack-4.1.0:\n",
            "      Successfully uninstalled hpack-4.1.0\n",
            "  Attempting uninstall: h11\n",
            "    Found existing installation: h11 0.16.0\n",
            "    Uninstalling h11-0.16.0:\n",
            "      Successfully uninstalled h11-0.16.0\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: h2\n",
            "    Found existing installation: h2 4.3.0\n",
            "    Uninstalling h2-4.3.0:\n",
            "      Successfully uninstalled h2-4.3.0\n",
            "  Attempting uninstall: httpcore\n",
            "    Found existing installation: httpcore 1.0.9\n",
            "    Uninstalling httpcore-1.0.9:\n",
            "      Successfully uninstalled httpcore-1.0.9\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "mcp 1.14.1 requires httpx>=0.27.1, but you have httpx 0.13.3 which is incompatible.\n",
            "openai 1.108.0 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
            "gradio-client 1.13.0 requires httpx>=0.24.1, but you have httpx 0.13.3 which is incompatible.\n",
            "gradio 5.46.0 requires httpx<1.0,>=0.24.1, but you have httpx 0.13.3 which is incompatible.\n",
            "firebase-admin 6.9.0 requires httpx[http2]==0.28.1, but you have httpx 0.13.3 which is incompatible.\n",
            "google-genai 1.38.0 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.13.3 which is incompatible.\n",
            "langsmith 0.4.28 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2025.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 jaro-winkler-2.0.3 rfc3986-1.5.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Étape 2 : Saisie utilisateur & traductions**"
      ],
      "metadata": {
        "id": "1COF1-PG6ZNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_en = input(\"Entrez un paragraphe en anglais (≥ 3 phrases) :\\n> \")\n",
        "\n",
        "# traduction manuelle par l'utilisateur\n",
        "manual_fr = input(\"\\nTraduction manuelle en français :\\n> \")\n",
        "\n",
        "# traduction automatique\n",
        "translator = Translator()\n",
        "auto_fr = translator.translate(text_en, src='en', dest='fr').text\n",
        "\n",
        "print(\"\\n=== Textes obtenus ===\")\n",
        "print(\"Texte original (EN) :\", text_en)\n",
        "print(\"Traduction manuelle :\", manual_fr)\n",
        "print(\"Traduction auto (Google) :\", auto_fr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuBFTBgA6sJJ",
        "outputId": "bc06487d-58b6-486e-fd1a-f503a1678360"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entrez un paragraphe en anglais (≥ 3 phrases) :\n",
            "> Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book.\n",
            "\n",
            "Traduction manuelle en français :\n",
            "> Le Lorem Ipsum est simplement du faux texte employé dans la composition et la mise en page avant impression. Il est le faux texte standard de l'industrie depuis le XVIe siècle, lorsqu'un imprimeur anonyme assembla une galée de caractères pour en faire un recueil de spécimens typographiques.\n",
            "\n",
            "=== Textes obtenus ===\n",
            "Texte original (EN) : Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book.\n",
            "Traduction manuelle : Le Lorem Ipsum est simplement du faux texte employé dans la composition et la mise en page avant impression. Il est le faux texte standard de l'industrie depuis le XVIe siècle, lorsqu'un imprimeur anonyme assembla une galée de caractères pour en faire un recueil de spécimens typographiques.\n",
            "Traduction auto (Google) : Lorem Ipsum est simplement un texte muet de l'industrie de l'impression et de la composition.Lorem Ipsum est le texte manqué standard de l'industrie depuis les années 1500, lorsqu'une imprimante inconnue a pris une cuisine de type et l'a brouillée pour faire un livre de spécimen de type.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Étape 3 : Similarité syntaxique**"
      ],
      "metadata": {
        "id": "Jhnmgyf-7NmK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1 Jaro-Winkler (texte entier)**"
      ],
      "metadata": {
        "id": "YLUjX5os7THo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "score_jaro = jaro.jaro_winkler_metric(manual_fr, auto_fr)\n",
        "print(f\"\\n[Jaro-Winkler] Similarité globale : {score_jaro:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nxnXFOX7WPg",
        "outputId": "076d56e2-3670-4c7c-f566-1e359bc627a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Jaro-Winkler] Similarité globale : 0.804\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2 Similarité par phrases (Jaccard + Jaro)**"
      ],
      "metadata": {
        "id": "MqMnb-9e7cN4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On découpe les deux textes en phrases, puis on compare phrase par phrase avec Jaro-Winkler.\n",
        "Ensuite, on applique le coefficient de Jaccard."
      ],
      "metadata": {
        "id": "-HR8dikW7jWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def jaccard(text1, text2, theta=0.85):\n",
        "    sents1 = sent_tokenize(text1)\n",
        "    sents2 = sent_tokenize(text2)\n",
        "\n",
        "    matches = 0\n",
        "    total = len(sents1) + len(sents2)\n",
        "\n",
        "    for s1 in sents1:\n",
        "        for s2 in sents2:\n",
        "            if jaro.jaro_winkler_metric(s1, s2) > theta:\n",
        "                matches += 1\n",
        "                break  # éviter double comptage\n",
        "\n",
        "    return matches / (total - matches)\n",
        "\n",
        "jac_score = jaccard(manual_fr, auto_fr)\n",
        "print(f\"[Jaccard sur phrases] Similarité : {jac_score:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Eoi9hny7lnl",
        "outputId": "bd5f8bba-e502-45ae-e710-c7ad72eedb72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Jaccard sur phrases] Similarité : 0.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Étape 4 : Similarité sémantique**"
      ],
      "metadata": {
        "id": "nzVO5QaB8Cnr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On compare mot par mot en cherchant leur sens avec Lesk, puis on calcule la similarité avec Wu-Palmer (wup_similarity)."
      ],
      "metadata": {
        "id": "NDcppnmW8giS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_synsets(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    synsets = []\n",
        "    for token in tokens:\n",
        "        syn = lesk(tokens, token)\n",
        "        if syn:\n",
        "            synsets.append(syn)\n",
        "    return synsets\n",
        "\n",
        "def semantic_distance(text1, text2):\n",
        "    syn1 = get_synsets(text1)\n",
        "    syn2 = get_synsets(text2)\n",
        "\n",
        "    scores = []\n",
        "    for s1 in syn1:\n",
        "        best = 0\n",
        "        for s2 in syn2:\n",
        "            sim = s1.wup_similarity(s2)\n",
        "            if sim and sim > best:\n",
        "                best = sim\n",
        "        if best > 0:\n",
        "            scores.append(best)\n",
        "    return sum(scores) / len(scores) if scores else 0\n",
        "\n",
        "sem_score = semantic_distance(manual_fr, auto_fr)\n",
        "print(f\"\\n[Wu-Palmer] Similarité sémantique : {sem_score:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10B4x-Pc8jbQ",
        "outputId": "f7537bb1-960f-4e72-ebc8-18343a02cc0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Wu-Palmer] Similarité sémantique : 0.800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Résumé***"
      ],
      "metadata": {
        "id": "LHLx9UVt9g8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n====== Résumé de la comparaison ======\\n\")\n",
        "print(\"Texte original (EN) :\", text_en)\n",
        "print(\"Traduction manuelle :\", manual_fr)\n",
        "print(\"Traduction auto (Google) :\", auto_fr)\n",
        "\n",
        "print(\"\\n--- Scores de similarité ---\")\n",
        "print(f\"Similarité syntaxique (Jaro-Winkler global) : {score_jaro:.3f}\")\n",
        "print(f\"Similarité syntaxique (Jaccard sur phrases) : {jac_score:.3f}\")\n",
        "print(f\"Similarité sémantique (Wu-Palmer via Lesk) : {sem_score:.3f}\")\n",
        "\n",
        "print(\"\\n--- Analyse rapide ---\")\n",
        "# Analyse Jaro\n",
        "if score_jaro > 0.85:\n",
        "    print(\"✅ Les deux traductions sont très proches au niveau syntaxique (Jaro-Winkler).\")\n",
        "else:\n",
        "    print(\"⚠️ Différences notables dans la structure des phrases (Jaro-Winkler).\")\n",
        "\n",
        "# Analyse Jaccard\n",
        "if jac_score > 0.75:\n",
        "    print(\"✅ Les traductions contiennent majoritairement les mêmes phrases (Jaccard).\")\n",
        "else:\n",
        "    print(\"⚠️ Plusieurs phrases diffèrent entre les deux traductions (Jaccard).\")\n",
        "\n",
        "# Analyse sémantique\n",
        "if sem_score > 0.75:\n",
        "    print(\"✅ Le sens général est bien conservé (forte similarité sémantique).\")\n",
        "else:\n",
        "    print(\"⚠️ Possible divergence de sens entre les deux traductions (sémantique).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4s2RFuE9XZ-",
        "outputId": "5a0ed804-5372-4959-dc97-fddb6a15655a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "====== Résumé de la comparaison ======\n",
            "\n",
            "Texte original (EN) : Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book.\n",
            "Traduction manuelle : Le Lorem Ipsum est simplement du faux texte employé dans la composition et la mise en page avant impression. Il est le faux texte standard de l'industrie depuis le XVIe siècle, lorsqu'un imprimeur anonyme assembla une galée de caractères pour en faire un recueil de spécimens typographiques.\n",
            "Traduction auto (Google) : Lorem Ipsum est simplement un texte muet de l'industrie de l'impression et de la composition.Lorem Ipsum est le texte manqué standard de l'industrie depuis les années 1500, lorsqu'une imprimante inconnue a pris une cuisine de type et l'a brouillée pour faire un livre de spécimen de type.\n",
            "\n",
            "--- Scores de similarité ---\n",
            "Similarité syntaxique (Jaro-Winkler global) : 0.804\n",
            "Similarité syntaxique (Jaccard sur phrases) : 0.000\n",
            "Similarité sémantique (Wu-Palmer via Lesk) : 0.800\n",
            "\n",
            "--- Analyse rapide ---\n",
            "⚠️ Différences notables dans la structure des phrases (Jaro-Winkler).\n",
            "⚠️ Plusieurs phrases diffèrent entre les deux traductions (Jaccard).\n",
            "✅ Le sens général est bien conservé (forte similarité sémantique).\n"
          ]
        }
      ]
    }
  ]
}